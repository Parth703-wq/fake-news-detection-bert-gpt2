# -*- coding: utf-8 -*-
"""NEWS_PRED_DAY2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PhBkwe54g3xOs--W-0VXxlKhBYdYhd2m
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# 1. Load datasets from CSV and Excel
df_csv = pd.read_csv("/content/drive/MyDrive/IFND.csv", encoding="latin1")
df_xls = pd.read_excel("/content/drive/MyDrive/fakenewsincidents_india.xls")

# 2. Preprocess IFND.csv
df_csv = df_csv[["Statement", "Label"]].dropna()  # Keep only the columns we need and remove rows with missing values
df_csv["label"] = df_csv["Label"].map({"TRUE": 0, "FAKE": 1})  # Convert textual labels to binary (0 for TRUE, 1 for FAKE)
df_csv = df_csv.rename(columns={"Statement": "text"})  # Rename 'Statement' column to 'text'
df_csv = df_csv[["text", "label"]]  # Final cleaned structure

# 3. Preprocess Excel file
# Use 'Content' column; if missing, use 'fake_news_title'
df_xls["text"] = df_xls["Content"].fillna(df_xls["fake_news_title"])
df_xls["label"] = 1  # All entries in this dataset are fake, so label = 1
df_xls = df_xls[["text", "label"]].dropna()  # Remove any rows with missing values

# 4. Combine both datasets into one
df_all = pd.concat([df_csv, df_xls], ignore_index=True)  # Merge datasets into one
df_all = df_all.dropna(subset=["text", "label"])  # Final cleanup just in case

# 5. Split the data into train and test sets
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df_all["text"].tolist(),               # Input text data
    df_all["label"].tolist(),             # Labels (0 or 1)
    test_size=0.2,                        # 20% data for testing
    random_state=42,                      # For reproducibility
    stratify=df_all["label"]              # Ensures class balance in splits
)

# Import tokenizers
from transformers import BertTokenizer, GPT2Tokenizer

# Load pretrained tokenizers
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token  # Set pad token to EOS for GPT2

MAX_LEN = 128  # Maximum token length

# Function to tokenize/encode text for input to models
def encode(tokenizer, texts):
    return tokenizer(
        texts,
        padding="max_length",       # Pad shorter sentences
        truncation=True,            # Truncate longer ones
        max_length=MAX_LEN,
        return_tensors='tf'         # Return as TensorFlow tensors
    )

# Tokenize training and test data for both models
train_bert = encode(bert_tokenizer, train_texts)
test_bert = encode(bert_tokenizer, test_texts)
train_gpt2 = encode(gpt2_tokenizer, train_texts)
test_gpt2 = encode(gpt2_tokenizer, test_texts)

print("ðŸ“¦ Total dataset samples:", len(df_all))
print("ðŸ§ª Total test samples:", len(test_labels))
print("ðŸ§‘â€ðŸ« Total training samples:", len(train_labels))

# Load necessary modules
import tensorflow as tf
from transformers import TFBertModel, TFGPT2Model

# Load pretrained transformer models
bert_model = TFBertModel.from_pretrained('bert-base-uncased')
gpt2_model = TFGPT2Model.from_pretrained('gpt2')

# Define input layers for both models
input_ids_bert = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name="bert_input_ids")
attention_mask_bert = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name="bert_attention_mask")
input_ids_gpt2 = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name="gpt2_input_ids")
attention_mask_gpt2 = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name="gpt2_attention_mask")

# Pass inputs through BERT
bert_outputs = bert_model(input_ids=input_ids_bert, attention_mask=attention_mask_bert)
bert_pooled = bert_outputs.pooler_output  # Extract final pooled output (representation of whole sentence)

# Pass inputs through GPT2
gpt2_outputs = gpt2_model(input_ids=input_ids_gpt2, attention_mask=attention_mask_gpt2)
gpt2_last = gpt2_outputs.last_hidden_state[:, -1, :]  # Take last token's hidden state (GPT doesn't have pooler)

# Combine outputs from both models
concat = tf.keras.layers.Concatenate()([bert_pooled, gpt2_last])
x = tf.keras.layers.Dense(64, activation='relu')(concat)  # Hidden dense layer
x = tf.keras.layers.Dropout(0.2)(x)                       # Dropout for regularization
output = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # Final output layer for binary classification

# Build final model
model = tf.keras.Model(
    inputs=[input_ids_bert, attention_mask_bert, input_ids_gpt2, attention_mask_gpt2],
    outputs=output
)

# Build the model shape
model.build(input_shape=[(None, MAX_LEN), (None, MAX_LEN), (None, MAX_LEN), (None, MAX_LEN)])

# Compile the model with optimizer, loss function, and evaluation metric
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Print model architecture summary
model.summary()

# Import progress bar callback for training
from tqdm.keras import TqdmCallback

# Set training hyperparameters
BATCH_SIZE = 8
EPOCHS = 2

# Train the model
history = model.fit(
    [
        train_bert["input_ids"], train_bert["attention_mask"],
        train_gpt2["input_ids"], train_gpt2["attention_mask"]
    ],
    np.array(train_labels),  # Labels for training
    validation_data = (
        [
            test_bert["input_ids"], test_bert["attention_mask"],
            test_gpt2["input_ids"], test_gpt2["attention_mask"]
        ],
        np.array(test_labels)  # Labels for validation
    ),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=[TqdmCallback(verbose=1)]  # Show training progress bar
)

model.save('/content/drive/MyDrive/gbert_hybrid_savedmodel')

from google.colab import files
import shutil
import os

# Define the path to the saved model directory
model_dir = '/content/drive/MyDrive/gbert_hybrid_savedmodel'

# Define the desired zip file name
zip_filename = '/content/drive/MyDrive/gbert_hybrid_savedmodel.zip'

# Create a zip archive of the saved model directory
shutil.make_archive(model_dir, 'zip', model_dir)

# Check if the zip file was created
if os.path.exists(zip_filename):
  # Download the zip file
  files.download(zip_filename)
else:
  print(f"Error: Zip file not created at {zip_filename}")

# Evaluate the model on test data
from sklearn.metrics import classification_report

# Get predictions on test set
preds = model.predict([
    test_bert["input_ids"], test_bert["attention_mask"],
    test_gpt2["input_ids"], test_gpt2["attention_mask"]
])

# Convert probabilities to binary labels (0 or 1)
pred_labels = (preds.flatten() > 0.5).astype(int)

# Print precision, recall, f1-score, accuracy, etc.
print(classification_report(test_labels, pred_labels, digits=4))

# Print validation accuracy after each epoch
print(history.history['val_accuracy'])

# Plot training vs validation accuracy graph
import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Train vs Validation Accuracy')
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(test_labels, pred_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["TRUE", "FAKE"])
disp.plot(cmap="Blues")
plt.title("Confusion Matrix")
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# Step 1: Sample 100 random news articles from your combined dataset
random_sample = df_all.sample(n=100, random_state=42)
random_texts = random_sample['text'].tolist()
true_labels = random_sample['label'].tolist()

# Step 2: Encode using both BERT and GPT-2 tokenizers
encoded_bert = encode(bert_tokenizer, random_texts)
encoded_gpt2 = encode(gpt2_tokenizer, random_texts)

# Step 3: Predict using the trained model
preds = model.predict([
    encoded_bert["input_ids"], encoded_bert["attention_mask"],
    encoded_gpt2["input_ids"], encoded_gpt2["attention_mask"]
])

# Convert probabilities to 0 or 1 (threshold = 0.5)
predicted_labels = (preds.flatten() > 0.5).astype(int)

# Step 4: Evaluation Metrics
print("\nClassification Report (on 100 Random Samples):\n")
print(classification_report(true_labels, predicted_labels, target_names=["TRUE", "FAKE"], digits=4))

# Step 5: Confusion Matrix
cm = confusion_matrix(true_labels, predicted_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["TRUE", "FAKE"])
disp.plot(cmap="Blues")
plt.title("Confusion Matrix (100 Random News Samples)")
plt.show()

# Step 6: Show first 10 predictions with actual vs predicted
print("\nðŸ” First 10 News Predictions:")
for i in range(10):
    print(f"\nNews {i+1}: {random_texts[i][:150]}...")  # show only first 150 chars
    print(f"Actual Label   : {'FAKE' if true_labels[i] == 1 else 'TRUE'}")
    print(f"Predicted Label: {'FAKE' if predicted_labels[i] == 1 else 'TRUE'}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    accuracy_score,
    f1_score
)

# Step 1: Randomly sample 500 entries from the dataset
sample_500 = df_all.sample(n=500, random_state=123)
sample_texts = sample_500["text"].tolist()
true_labels_500 = sample_500["label"].tolist()

# Step 2: Tokenize for BERT and GPT-2
encoded_bert_500 = encode(bert_tokenizer, sample_texts)
encoded_gpt2_500 = encode(gpt2_tokenizer, sample_texts)

# Step 3: Predict
preds_500 = model.predict([
    encoded_bert_500["input_ids"], encoded_bert_500["attention_mask"],
    encoded_gpt2_500["input_ids"], encoded_gpt2_500["attention_mask"]
])
predicted_labels_500 = (preds_500.flatten() > 0.5).astype(int)

# Step 4: Evaluation Metrics
print("\nðŸ“Š Classification Report (500 samples):")
print(classification_report(true_labels_500, predicted_labels_500, target_names=["TRUE", "FAKE"], digits=4))

# Step 5: Accuracy & F1 Score
accuracy = accuracy_score(true_labels_500, predicted_labels_500)
f1 = f1_score(true_labels_500, predicted_labels_500)
print(f"\nâœ… Accuracy: {accuracy:.4f}")
print(f"âœ… F1 Score: {f1:.4f}")

# Step 6: Confusion Matrix
cm_500 = confusion_matrix(true_labels_500, predicted_labels_500)
disp_500 = ConfusionMatrixDisplay(confusion_matrix=cm_500, display_labels=["TRUE", "FAKE"])
disp_500.plot(cmap="Purples")
plt.title("Confusion Matrix - 500 Sample Test")
plt.show()

# Step 7: Display first 10 predictions for inspection
print("\nðŸ” Sample Predictions:")
for i in range(10):
    print(f"\nNews {i+1}: {sample_texts[i][:150]}...")
    print(f"Actual   : {'FAKE' if true_labels_500[i]==1 else 'TRUE'}")
    print(f"Predicted: {'FAKE' if predicted_labels_500[i]==1 else 'TRUE'}")

# Custom news samples you want to test
custom_news = [
    "The government has announced a new policy to reduce petrol prices across the country.",
    "Aliens have landed in Mumbai and are building a secret base under the city.",
    "India will host the next Olympic Games in 2028, says the official committee.",
    "A woman gave birth to 10 babies at once in Delhi hospital, doctors shocked!",
    "ISRO successfully launches a satellite to study black holes in collaboration with NASA."
]

# Step 1: Tokenize the custom news using both tokenizers
encoded_custom_bert = encode(bert_tokenizer, custom_news)
encoded_custom_gpt2 = encode(gpt2_tokenizer, custom_news)

# Step 2: Predict
custom_preds = model.predict([
    encoded_custom_bert["input_ids"], encoded_custom_bert["attention_mask"],
    encoded_custom_gpt2["input_ids"], encoded_custom_gpt2["attention_mask"]
])

# Step 3: Interpret predictions
custom_labels = (custom_preds.flatten() > 0.5).astype(int)

# Step 4: Display results
print("\nðŸ“° Custom News Predictions:\n")
for i, news in enumerate(custom_news):
    label = "FAKE" if custom_labels[i] == 1 else "TRUE"
    print(f"News {i+1}: {news}")
    print(f"Prediction: {label}\n")

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
files.download('/content/drive/MyDrive/gbert_hybrid_savedmodel.zip')

